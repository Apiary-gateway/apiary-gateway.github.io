<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>APIARY</title><link rel="icon" href="/images/apiary_favicon.png"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Gotham:wght@700&display=swap" rel="stylesheet"><style>.image-container[data-astro-cid-6kov3kig]{margin:2rem 0;display:flex;flex-direction:column;align-items:center}.image[data-astro-cid-6kov3kig]{height:v-bind(height);width:v-bind(width);max-width:100%;transition:transform .2s ease;border-radius:4px;cursor:pointer;margin:0;padding:0;display:block;object-fit:contain}.image[data-astro-cid-6kov3kig]:hover{transform:scale(1.02);box-shadow:0 0 10px #cd9e054d}.image-caption[data-astro-cid-6kov3kig]{text-align:center;margin-top:.5rem;max-width:80ch;margin-left:auto;margin-right:auto;padding:0 1rem;font-size:.9rem;color:var(--color-text);opacity:.8}.modal[data-astro-cid-6kov3kig]{display:none;position:fixed;z-index:1000;top:0;left:0;width:100%;height:100%;background-color:#000000d9;overflow:auto}.modal-content[data-astro-cid-6kov3kig]{position:relative;margin:auto;padding:2rem;width:90%;max-width:1200px;display:flex;flex-direction:column;align-items:center;justify-content:center;min-height:100vh;background:linear-gradient(135deg,#2d2d2d,#1a1a1a);border-radius:8px}#modalImage[data-astro-cid-6kov3kig]{max-width:100%;max-height:80vh;object-fit:contain;margin:1rem 0;border-radius:4px}#modalCaption[data-astro-cid-6kov3kig]{color:#f0d264;text-align:center;padding:1rem;max-width:80%;font-size:1.1rem;text-shadow:0 1px 2px rgba(0,0,0,.5)}.close[data-astro-cid-6kov3kig]{position:absolute;top:1rem;right:1rem;color:#f0d264;font-size:40px;font-weight:700;transition:.3s;cursor:pointer;z-index:1001;text-shadow:0 1px 2px rgba(0,0,0,.5)}.close[data-astro-cid-6kov3kig]:hover{color:#cd9e05;transform:scale(1.1)}
</style>
<link rel="stylesheet" href="/_astro/case-study.IE2zUtd3.css"></head> <body data-astro-cid-37fxchfa> <nav class="navbar" data-astro-cid-5blmo7yk> <div class="navbar-container" data-astro-cid-5blmo7yk> <div class="logo-container" data-astro-cid-5blmo7yk> <button class="hamburger" aria-label="Toggle menu" data-astro-cid-5blmo7yk> <span data-astro-cid-5blmo7yk></span> <span data-astro-cid-5blmo7yk></span> <span data-astro-cid-5blmo7yk></span> </button> <a href="/" class="logo" data-astro-cid-5blmo7yk> <img src="/apiary-logo.png" alt="Apiary Logo" data-astro-cid-5blmo7yk> </a> </div> <div class="nav-links" data-astro-cid-5blmo7yk> <a href="/case-study" class="active" data-astro-cid-5blmo7yk>Case Study</a> <a href="/docs" class data-astro-cid-5blmo7yk>Docs</a> <a href="/#team" class data-astro-cid-5blmo7yk>Team</a> <a href="https://github.com/Sporkway" target="_blank" class="external-link" data-astro-cid-5blmo7yk>
Github
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-5blmo7yk> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6" data-astro-cid-5blmo7yk></path> <polyline points="15 3 21 3 21 9" data-astro-cid-5blmo7yk></polyline> <line x1="10" y1="14" x2="21" y2="3" data-astro-cid-5blmo7yk></line> </svg> </a> <button class="theme-toggle" aria-label="Toggle theme" data-astro-cid-5blmo7yk> <svg id="theme-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-5blmo7yk></svg> </button> </div> </div> <div class="mobile-menu" data-astro-cid-5blmo7yk> <button class="close-btn" aria-label="Close menu" data-astro-cid-5blmo7yk>×</button> <div class="mobile-menu-content" data-astro-cid-5blmo7yk> <a href="/case-study" class="active" data-astro-cid-5blmo7yk>Case Study</a> <hr data-astro-cid-5blmo7yk> <a href="/docs" class data-astro-cid-5blmo7yk>Docs</a> <hr data-astro-cid-5blmo7yk> <a href="/team" class data-astro-cid-5blmo7yk>Team</a> <hr data-astro-cid-5blmo7yk> <a href="/contact" class="external-link" data-astro-cid-5blmo7yk>
Github
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-5blmo7yk> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6" data-astro-cid-5blmo7yk></path> <polyline points="15 3 21 3 21 9" data-astro-cid-5blmo7yk></polyline> <line x1="10" y1="14" x2="21" y2="3" data-astro-cid-5blmo7yk></line> </svg> </a> <hr data-astro-cid-5blmo7yk> <button class="theme-toggle" aria-label="Toggle theme" data-astro-cid-5blmo7yk> <svg id="theme-icon-mobile" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-5blmo7yk></svg> </button> </div> </div> </nav>  <script type="module">const a=document.querySelector(".hamburger"),c=document.querySelector(".mobile-menu"),y=document.querySelector(".close-btn"),i=document.querySelectorAll(".theme-toggle"),n=window.matchMedia("(prefers-color-scheme: dark)"),o={sun:`
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    `,moon:`
      <path d="M21 12.79A9 9 0 1 1 11.21 3A7 7 0 0 0 21 12.79z"></path>
    `},r=t=>{document.documentElement.setAttribute("data-theme",t),localStorage.setItem("theme",t),d(t),m(t)},d=t=>{const e=t==="dark"?o.sun:o.moon;document.querySelectorAll("#theme-icon, #theme-icon-mobile").forEach(s=>{s.innerHTML=e})},m=t=>{const e=document.documentElement;t==="dark"?(e.style.setProperty("--navbar-bg","#1a1a1a"),e.style.setProperty("--navbar-border","#2d2d2d"),e.style.setProperty("--text-primary","#ffffff"),e.style.setProperty("--primary-color","#cd9e05"),e.style.setProperty("--color-background","#1a1a1a"),e.style.setProperty("--color-background-alt","#242424"),e.style.setProperty("--color-text","#ffffff"),e.style.setProperty("--color-border","#404040")):(e.style.setProperty("--navbar-bg","#ffffff"),e.style.setProperty("--navbar-border","#e0e0e0"),e.style.setProperty("--text-primary","#333333"),e.style.setProperty("--primary-color","#cd9e05"),e.style.setProperty("--color-background","#ffffff"),e.style.setProperty("--color-background-alt","#f1f5f9"),e.style.setProperty("--color-text","#333333"),e.style.setProperty("--color-border","#e0e0e0"))},f=()=>{const t=document.documentElement.getAttribute("data-theme");r(t==="dark"?"light":"dark")},l=localStorage.getItem("theme");r(l||(n.matches?"dark":"light"));n.addEventListener("change",t=>{localStorage.getItem("theme")||r(t.matches?"dark":"light")});i.forEach(t=>{t.addEventListener("click",f)});a.addEventListener("click",()=>{c.classList.add("active")});y.addEventListener("click",()=>{c.classList.remove("active")});</script> <main class="main-content" data-astro-cid-37fxchfa>  <div class="layout" data-astro-cid-a72tx4qi> <aside class="sidebar" data-astro-cid-a72tx4qi> <nav data-astro-cid-a72tx4qi> <ul class="toc-list" data-astro-cid-a72tx4qi> <li data-astro-cid-a72tx4qi> <a href="#introduction" class="toc-link" id="link-introduction" data-astro-cid-a72tx4qi> Introduction </a> <ul class="toc-sublist" data-astro-cid-a72tx4qi> <li data-astro-cid-a72tx4qi> <a href="#what-is-apiary" class="toc-link" id="link-what-is-apiary" data-astro-cid-a72tx4qi> What is Apiary? </a> </li> </ul> </li><li data-astro-cid-a72tx4qi> <a href="#background" class="toc-link" id="link-background" data-astro-cid-a72tx4qi> Background </a> <ul class="toc-sublist" data-astro-cid-a72tx4qi> <li data-astro-cid-a72tx4qi> <a href="#foundation-models" class="toc-link" id="link-foundation-models" data-astro-cid-a72tx4qi> Foundation Models and Model-as-a-Service </a> </li><li data-astro-cid-a72tx4qi> <a href="#challenges" class="toc-link" id="link-challenges" data-astro-cid-a72tx4qi> Challenges of Working with LLMs </a> </li><li data-astro-cid-a72tx4qi> <a href="#multiple-llms" class="toc-link" id="link-multiple-llms" data-astro-cid-a72tx4qi> Why Use Multiple LLMs? </a> </li><li data-astro-cid-a72tx4qi> <a href="#challenges-of-using-multiple-llms" class="toc-link" id="link-challenges-of-using-multiple-llms" data-astro-cid-a72tx4qi> Challenges of Using Multiple LLMs </a> </li><li data-astro-cid-a72tx4qi> <a href="#abstraction" class="toc-link" id="link-abstraction" data-astro-cid-a72tx4qi> A New Layer of Abstraction </a> </li> </ul> </li><li data-astro-cid-a72tx4qi> <a href="#existing-solutions" class="toc-link" id="link-existing-solutions" data-astro-cid-a72tx4qi> Existing Solutions </a> <ul class="toc-sublist" data-astro-cid-a72tx4qi> <li data-astro-cid-a72tx4qi> <a href="#open-source-tools" class="toc-link" id="link-open-source-tools" data-astro-cid-a72tx4qi> Open Source Tools </a> </li><li data-astro-cid-a72tx4qi> <a href="#managed-services" class="toc-link" id="link-managed-services" data-astro-cid-a72tx4qi> Managed Services </a> </li><li data-astro-cid-a72tx4qi> <a href="#tradeoffs" class="toc-link" id="link-tradeoffs" data-astro-cid-a72tx4qi> Tradeoffs </a> </li> </ul> </li><li data-astro-cid-a72tx4qi> <a href="#our-solution" class="toc-link" id="link-our-solution" data-astro-cid-a72tx4qi> Our Solution </a> <ul class="toc-sublist" data-astro-cid-a72tx4qi> <li data-astro-cid-a72tx4qi> <a href="#differentiators" class="toc-link" id="link-differentiators" data-astro-cid-a72tx4qi> Differentiators </a> </li><li data-astro-cid-a72tx4qi> <a href="#who-apiary-is-for" class="toc-link" id="link-who-apiary-is-for" data-astro-cid-a72tx4qi> Who Apiary Is For </a> </li><li data-astro-cid-a72tx4qi> <a href="#using-apiary" class="toc-link" id="link-using-apiary" data-astro-cid-a72tx4qi> Using Apiary </a> </li><li data-astro-cid-a72tx4qi> <a href="#a-walkthrough-of-the-apiary-dashboard" class="toc-link" id="link-a-walkthrough-of-the-apiary-dashboard" data-astro-cid-a72tx4qi> A Walkthrough of the Apiary Dashboard </a> </li> </ul> </li><li data-astro-cid-a72tx4qi> <a href="#building-apiary" class="toc-link" id="link-building-apiary" data-astro-cid-a72tx4qi> Building Apiary </a> <ul class="toc-sublist" data-astro-cid-a72tx4qi> <li data-astro-cid-a72tx4qi> <a href="#serverless-architecture" class="toc-link" id="link-serverless-architecture" data-astro-cid-a72tx4qi> Serverless Architecture </a> </li><li data-astro-cid-a72tx4qi> <a href="#request-response-transformation" class="toc-link" id="link-request-response-transformation" data-astro-cid-a72tx4qi> Request/Response Transformation </a> </li><li data-astro-cid-a72tx4qi> <a href="#api-key-management" class="toc-link" id="link-api-key-management" data-astro-cid-a72tx4qi> API Key Management </a> </li><li data-astro-cid-a72tx4qi> <a href="#managing-conversation-history" class="toc-link" id="link-managing-conversation-history" data-astro-cid-a72tx4qi> Managing Conversation History </a> </li><li data-astro-cid-a72tx4qi> <a href="#configurable-routing" class="toc-link" id="link-configurable-routing" data-astro-cid-a72tx4qi> Configurable Routing </a> </li><li data-astro-cid-a72tx4qi> <a href="#caching" class="toc-link" id="link-caching" data-astro-cid-a72tx4qi> Caching </a> </li><li data-astro-cid-a72tx4qi> <a href="#guardrails" class="toc-link" id="link-guardrails" data-astro-cid-a72tx4qi> Guardrails </a> </li><li data-astro-cid-a72tx4qi> <a href="#collecting-and-storing-observability-data" class="toc-link" id="link-collecting-and-storing-observability-data" data-astro-cid-a72tx4qi> Collecting and Storing Observability Data </a> </li><li data-astro-cid-a72tx4qi> <a href="#observability-and-management-dashboard" class="toc-link" id="link-observability-and-management-dashboard" data-astro-cid-a72tx4qi> Observability and Management Dashboard </a> </li> </ul> </li><li data-astro-cid-a72tx4qi> <a href="#future-work" class="toc-link" id="link-future-work" data-astro-cid-a72tx4qi> Future Work </a> <ul class="toc-sublist" data-astro-cid-a72tx4qi> <li data-astro-cid-a72tx4qi> <a href="#improving-resiliency" class="toc-link" id="link-improving-resiliency" data-astro-cid-a72tx4qi> Improving Resiliency </a> </li><li data-astro-cid-a72tx4qi> <a href="#user-querying" class="toc-link" id="link-user-querying" data-astro-cid-a72tx4qi> User Querying </a> </li><li data-astro-cid-a72tx4qi> <a href="#optimizing-latency" class="toc-link" id="link-optimizing-latency" data-astro-cid-a72tx4qi> Optimizing Latency </a> </li><li data-astro-cid-a72tx4qi> <a href="#conversation-management" class="toc-link" id="link-conversation-management" data-astro-cid-a72tx4qi> Conversation Management </a> </li><li data-astro-cid-a72tx4qi> <a href="#additional-guardrail-features" class="toc-link" id="link-additional-guardrail-features" data-astro-cid-a72tx4qi> Additional Guardrail Features </a> </li> </ul> </li><li data-astro-cid-a72tx4qi> <a href="#references" class="toc-link" id="link-references" data-astro-cid-a72tx4qi> References </a> <ul class="toc-sublist" data-astro-cid-a72tx4qi>  </ul> </li> </ul> </nav> </aside> <main class="content" data-astro-cid-a72tx4qi> <div class="content-wrapper" data-astro-cid-a72tx4qi>   <section id="introduction"> <h2>Introduction</h2> <h3 id="what-is-apiary">What is Apiary?</h3> <p>Apiary is an open-source platform for managing requests to one or more large language models (LLMs) via a single API endpoint. Under the hood it can route requests to different LLMs, cache and reuse results, and apply “guardrails” to model outputs. Additionally,  it provides observability for every LLM call, allowing developers to view all of their applications’ LLM requests, responses, and costs in one central location.</p> <p>Developers can experiment and take advantage of the strengths of different models for different use cases within their applications, without having to manage multiple API formats or duplicate infrastructure for shared concerns like routing or guardrails.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/01-01-overview.png" alt="Apiary Overview" class="image" loading="lazy" data-src="/images/01-01-overview.png" data-alt="Apiary Overview" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Apiary Overview </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>  <script type="module">document.addEventListener("DOMContentLoaded",()=>{const l=document.querySelectorAll(".image"),t=document.getElementById("imageModal"),n=document.getElementById("modalImage"),o=document.getElementById("modalCaption"),d=document.querySelector(".close");!t||!n||!o||!d||(l.forEach(e=>{e.addEventListener("click",()=>{const c=e.getAttribute("data-src"),a=e.getAttribute("data-alt");c&&a&&(t.style.display="block",n.src=c,o.textContent=a)})}),d.addEventListener("click",()=>{t.style.display="none"}),window.addEventListener("click",e=>{e.target===t&&(t.style.display="none")}))});</script> </section><section id="background"> <h2>Background</h2> <p>Before further exploring the services that Apiary provides, it may be helpful to have more background on how LLMs may be used in applications and the challenges that can arise when integrating one or more LLMs. </p> <h3 id="foundation-models">Foundation Models and Model-as-a-Service</h3> <p>A foundation model, such as OpenAI's GPT-4, Anthropic's Claude, and Google's Gemini, is a general purpose AI model trained on large amounts of data and exposed via an API, enabling developers to integrate advanced AI capabilities into their applications.Typically, access to these models is provided via API calls to commercial Model-as-a-Service (MaaS) offerings.</p> <p>Foundation models are used for a wide variety of AI features in applications. Some common examples include:</p> <ul> <li>Retrieval-Augmented Generation (RAG) and custom chatbots</li> <li>Summarization of content such as documents, customer reviews, or academic papers</li> <li>Text generation, including product descriptions and blog posts</li> <li>Semantic search, using embeddings to retrieve and rank relevant documents</li> <li>Visual comprehension, such as image generation and identification</li> <li>Multi-modal systems, which are a combination of the above capabilities</li> </ul> <p>It's not uncommon for a single application to employ several of these use cases. Even the simplest AI application often involves multiple LLM calls - it may employ an LLM for classification of a query, as well as an embedding model to retrieve relevant documents, plus an LLM for the response generation, and finally a model for response evaluation. Integrating even one of these LLMs comes with complexity, which we'll discuss next.</p> <h3 id="challenges">Challenges of Working with LLMs</h3> <p>Working with LLMs has its own unique challenges:</p> <ul> <li><strong>Cost:</strong> API calls to LLMs can be expensive, especially for high-usage applications or lengthy queries.</li> <li><strong>High Latency:</strong> LLMs are typically slower than traditional services, particularly with large prompts or responses.</li> <li><strong>Unpredictability:</strong> LLMs are inherently non-deterministic and have the potential to generate undesirable responses, possibly exposing sensitive data, hallucinating information, or providing malicious content.</li> <li><strong>Availability:</strong> Leading models frequently experience downtime or degraded performance, often below the industry standard for high-availability systems.</li> <li><strong>Context Management:</strong> For chat applications, maintaining state and message history across requests adds a layer of complexity.</li> </ul> <p>As we discussed, even a simple AI application may have a need for using multiple LLMs. Before we get into the challenges of integrating more than one LLM into an application, we'll elaborate on some additional scenarios where more than one LLM may be necessary.</p> <h3 id="multiple-llms">Why Use Multiple LLMs?</h3> <h4>Performance and Cost Optimization</h4> <p>LLMs vary widely in price, performance, and behavior. A developer might route high-priority or complex tasks to a more powerful (and more expensive) model, while using smaller, open-source models for simpler or lower-stakes queries.</p> <p>For instance, Anthropic's Claude might be used for its natural, human-like writing style, while an open-source model may suffice for straightforward classification tasks. A chatbot for a car dealership might use a cheap, fast model to answer "What are your store hours?" and a more advanced model for "What would my monthly payment be if I finance this car over 4 years with $4,500 down?"</p> <h4>Reliability and Fallbacks</h4> <p>LLMs often fall short of enterprise reliability standards (e.g., "five nines" or 99.999% uptime). GPT-4 and Claude models, for example, typically offer only 99.2–99.7% availability <sup><a href="#footnote-1">1</a>, <a href="#footnote-2">2</a></sup>. In high-availability applications, developers must implement fallback strategies using alternative models to ensure uninterrupted service.</p> <h4>Performance Evaluation and Testing</h4> <p>During development or experimentation, teams often compare responses from different models to determine which performs best. In production, multiple models may be used simultaneously in A/B tests or canary deployments to evaluate response quality, bias, or relevance.</p> <p>"LLM as a judge", where a separate model evaluates the performance of a primary model, has also emerged as a highly popular way to evaluate LLM responses <sup><a href="#footnote-3">3</a></sup>.</p> <h4>Data Sensitivity and Internal Processing</h4> <p>Some organizations, especially in regulated industries, need to handle sensitive data internally. This may mean routing certain queries through self-hosted or internal models before allowing access to external APIs.</p> <h3 id="challenges-of-using-multiple-llms">Challenges of Using Multiple LLMs</h3> <p>While using multiple models can offer major benefits, it also significantly increases complexity. Developers must contend with:</p> <ul> <li><strong>Divergent request/response formats:</strong> Each provider has a unique API structure, which may change frequently. Even structured responses (e.g., JSON) can vary, requiring extra logic to parse and normalize them.</li> <li><strong>Different methods for handling context:</strong> Thread management and chat history differ between providers (e.g., OpenAI uses role-based message arrays, while Anthropic requires a system message per request).</li> <li><strong>Multiple API keys and credential management:</strong> Each model requires its own authentication and security setup.</li> <li><strong>Disparate observability systems:</strong> Without standardization, tracking cost, token usage, and response quality must be implemented separately for each provider.</li> <li><strong>Fallbacks and routing logic:</strong> Implementing conditional logic (e.g., routing based on cost, model failure, or task complexity) is error-prone and often duplicated.</li> </ul> <p>Finally, and perhaps most importantly, AI is a rapidly evolving field. Each of the above pain points is amplified by the fact that as API behavior, model availability, and pricing structures change, developers are forced to update the same logic all over the codebase. The sprawl becomes a constant, unmanageable burden.</p> <h3 id="abstraction">A New Layer of Abstraction</h3> <p>These challenges aren't going away. If anything, they're growing as LLMs are integrated into more and more applications. Rather than continuing to patch and duplicate logic across every integration, developers need a centralized layer to manage and abstract LLM interactions. This is where a <strong>model gateway</strong> comes in - a centralized layer that abstracts away the mess of LLM-integrated development.</p> <p>A model gateway stands between your application and one or more foundation model APIs. It centralizes routing, standardizes requests and response formats, manages credentials, tracks usage and cost, and can apply logic like retries, caching, and guardrails. To give an idea of what the interface for a model gateway can look like, the below image show's Cloudflare AI Gateway's dashboard. Each request is routed through the gateway, where it can be tracked and modified according to configured rules.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/02-01-cloudflare-ai-gateway-dashboard.png" alt="Cloudflare AI Gateway Dashboard" class="image" loading="lazy" data-src="/images/02-01-cloudflare-ai-gateway-dashboard.png" data-alt="Cloudflare AI Gateway Dashboard" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Cloudflare AI Gateway Dashboard </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   </section><section id="existing-solutions"> <h2>Existing Solutions</h2> <p>Several tools and platforms have emerged to serve as model gateways. We explored many of these tools in building our product. Each has tradeoffs, which helped us in clarifying what we wanted Apiary to become.</p> <h3 id="open-source-tools">Open Source Tools</h3> <ul> <li><strong>LiteLLM</strong> and <strong>Portkey (Open Source)</strong> are two open source tools that provide unified APIs for calling multiple LLM providers. Each handles request/response formatting and support basic routing logic. However, they do not offer infrastructure, observability tools, or advanced features like semantic caching or guardrails.</li> </ul> <h3 id="managed-services">Managed Services</h3> <ul> <li><strong>Cloudflare AI Gateway</strong> is a hosted gateway that provides a universal endpoint for LLM requests, basic caching, tracks token usage, and includes analytics. It runs entirely within Cloudflare’s infrastructure.</li> <li><strong>Gloo AI Gateway</strong> offers a managed control panel for LLM requests with support for routing, observability, rate-limiting, and basic content filters. It is built for production-grade deployments and runs on third-party infrastructure.</li> </ul> <h3 id="tradeoffs">Tradeoffs</h3> <ul> <li>The open source tools are lightweight and easy to integrate, but they lack infrastructure, observability, and have very limited feature sets.</li> <li>Both managed services, Cloudflare AI Gateway and Gloo AI Gateway, are fully managed and easier to use, but you have to give up data control and customization.</li> </ul> <p>Our ideal model gateway would be free to use, offer complete control over our data, provide configurable routing, request and response standardization, cost monitoring, semantic caching, guardrails, and observability. It also needed to be easy to deploy and fully configurable. With this clarity, we set out to build Apiary.</p> </section><section id="our-solution"> <h2>Our Solution</h2> <p>Apiary is a comprehensive, self-managed solution with built-in infrastructure. It's easy to deploy, and gives developers complete control over their data and infrastructure.</p> <h3 id="how-apiary-compares-to-alternatives">How Apiary Compares to Alternatives</h3> <p>In the table below, we've selected one open-source tool (Portkey) and one hosted solution (Cloudflare AI) to compare alongside Apiary.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-01-apiary-comparison-chart.png" alt="Apiary Comparison Chart" class="image" loading="lazy" data-src="/images/04-01-apiary-comparison-chart.png" data-alt="Apiary Comparison Chart" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Apiary Comparison Chart </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>As the chart illustrates, Apiary combines the strengths of both open-source flexibility and managed service convenience.</p> <p>Apiary differentiates itself from existing solutions through a unique combination of features:</p> <ul> <li> <strong>Complete Data Control</strong> <p>Unlike managed services like Cloudflare AI Gateway, Apiary runs entirely within your own AWS account, ensuring that sensitive prompt data and responses never leave your control.</p> </li> <li> <strong>Built-in Infrastructure</strong> <p>Unlike platforms like LiteLLM that provide only code, Apiary includes pre-configured serverless infrastructure that can be deployed with minimal setup, offering the ease of setup typically found in managed services, without the data sharing concerns.</p> </li> <li> <strong>Feature Set</strong> <p>Apiary includes advanced features that are often available only in premium tiers, or that require third-party integrations and configuration, when using existing offerings:</p> <ul> <li>Semantic caching, which compares the meaning of new requests to previous ones using vector embeddings</li> <li>Conversation thread management, for maintaining chat history across models</li> <li>Content guardrails, for detecting and managing undesirable responses</li> </ul> </li> <li> <strong>Cost Efficiency</strong> <p>Apiary is free and open-source, requiring only the cost of the AWS infrastructure it runs on and the fees charged by LLM providers.</p> </li> </ul> <h3 id="who-apiary-is-for">Who Apiary Is For</h3> <p>Apiary is built for small to medium-sized development teams working on AI-integrated applications where reliability, cost-efficiency, configurability, and observability are crucial. It provides a centralized point for managing aspects like API keys and guardrails, minimizing the coordination needed between different systems and team members. Built-in observability allows developers to track the life cycle of a request through the system, and monitor where errors may be occurring. Features like conditional routing, fallbacks, and caching help to optimize application performance and costs. Each of these features is configurable, allowing developers to optimize Apiary for their specific applications' needs.</p> <h3 id="a-walkthrough-of-the-apiary-dashboard">A Walkthrough of the Apiary Dashboard</h3> <p>Once deployed, Apiary provides users with two primary endpoints:</p> <ul> <li>A request endpoint where client applications can send prompts and receive responses from configured LLMs</li> <li>An observability and management dashboard where users can view their request logs and configure routing, caching, fallback, and guardrail settings through a user interface</li> </ul> <p>The Apiary user interface is designed to give developers full visibility of their LLM requests, as well as control over how the requests are routed. Here's a walkthrough of the key areas:</p> <h4>Logs Overview</h4> <p>The first page you'll see in the dashboard is the short-term logs view, which displays all of the requests and responses routed through Apiary in the past 5 days.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-02-short-term-logs.png" alt="Short Term Logs View" class="image" loading="lazy" data-src="/images/04-02-short-term-logs.png" data-alt="Short Term Logs View" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Short Term Logs Dashboard </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>You can then click on the hamburger icon in the upper-right and select "Show Long Term Logs" to retrieve the full historical logs for your application.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-03-dropdown-menu-focus-longterm-logs.png" alt="Show Long Term Logs Menu Option" class="image" loading="lazy" data-src="/images/04-03-dropdown-menu-focus-longterm-logs.png" data-alt="Show Long Term Logs Menu Option" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Show Long Term Logs Menu Option </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-04-long-term-logs.png" alt="Long Term Logs View" class="image" loading="lazy" data-src="/images/04-04-long-term-logs.png" data-alt="Long Term Logs View" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Long Term Logs Dashboard </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <h4>Configuring Request Handling and Routing</h4> <p>Apiary's routing behavior is controlled via a configuration object, which can be edited directly through the dashboard.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-05-dropdown-menu-focus-gateway-config.png" alt="Manage Config Object Menu" class="image" loading="lazy" data-src="/images/04-05-dropdown-menu-focus-gateway-config.png" data-alt="Manage Config Object Menu" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Manage Config Object Menu </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>The default configuration object is shown below, along with some notes on the configuration options:</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-06-manage-config-ui.png" alt="Manage Config UI" class="image" loading="lazy" data-src="/images/04-06-manage-config-ui.png" data-alt="Manage Config UI" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Manage Config UI </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>Conditional routing based on custom, user-defined fields can be configured through the configuration object in the <code>availableMetadata</code> and <code>conditions</code> arrays:</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-07-apiary-conditional-groups.png" alt="Apiary Conditional Groups" class="image" loading="lazy" data-src="/images/04-07-apiary-conditional-groups.png" data-alt="Apiary Conditional Groups" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Apiary Conditional Groups </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>In the above configuration, Apiary will look for a request header of "x-user-type," and if the value in that header is "pro," it will be routed to OpenAI's GPT-4 model 60% of the time, and Gemini's 1.5 Pro model 40% of the time.</p> <p>The dashboard provides syntax validation and helpful error messages to guide users through customizing the configuration object.</p> <h4>Setting Guardrails</h4> <p>The default Apiary configuration includes some basic guardrails, which can be expanded upon or edited through the user interface:</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-08-dropdown-menu-focus-guardrails.png" alt="Manage Guardrails Menu" class="image" loading="lazy" data-src="/images/04-08-dropdown-menu-focus-guardrails.png" data-alt="Manage Guardrails Menu" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Manage Guardrails Menu </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-09-guardrails-example.png" alt="Guardrails Example" class="image" loading="lazy" data-src="/images/04-09-guardrails-example.png" data-alt="Guardrails Example" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Guardrails Example </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>Apiary will check LLM responses, plus the context of the prompt, for semantic similarity to the configured guardrails. Any responses that fall above the configured threshold will be blocked. We provide a detailed discussion on guardrails and semantic similarity in the Building Apiary section of our case study.</p> <h4>Log Details</h4> <p>To see the exact settings for a given request, you can click on that request in the logs table and a details modal will open.</p> <div class="carousel-container" data-astro-cid-wfe7xcno> <div class="carousel" data-astro-cid-wfe7xcno> <div class="carousel-item" data-index="0" data-astro-cid-wfe7xcno> <img src="/images/04-10-log-details-part-1.png" alt="Log Details Part 1" data-astro-cid-wfe7xcno> <div class="caption" data-astro-cid-wfe7xcno>Log Details Part 1</div> </div><div class="carousel-item" data-index="1" data-astro-cid-wfe7xcno> <img src="/images/04-11-log-details-part-2.png" alt="Log Details Part 2" data-astro-cid-wfe7xcno> <div class="caption" data-astro-cid-wfe7xcno>Log Details Part 2</div> </div><div class="carousel-item" data-index="2" data-astro-cid-wfe7xcno> <img src="/images/04-12-routing-history-simple-cache-hit.png" alt="Model routing history displaying a simple cache hit." data-astro-cid-wfe7xcno> <div class="caption" data-astro-cid-wfe7xcno>Model routing history displaying a simple cache hit.</div> </div><div class="carousel-item" data-index="3" data-astro-cid-wfe7xcno> <img src="/images/04-13-routing-history-guardrail-hit.png" alt="Model routing history displaying a guardrail hit." data-astro-cid-wfe7xcno> <div class="caption" data-astro-cid-wfe7xcno>Model routing history displaying a guardrail hit.</div> </div> </div> <button class="carousel-control prev" aria-label="Previous slide" data-astro-cid-wfe7xcno>❮</button> <button class="carousel-control next" aria-label="Next slide" data-astro-cid-wfe7xcno>❯</button> <div class="carousel-indicators" data-astro-cid-wfe7xcno> <button class="indicator" data-index="0" aria-label="Go to slide 1" data-astro-cid-wfe7xcno></button><button class="indicator" data-index="1" aria-label="Go to slide 2" data-astro-cid-wfe7xcno></button><button class="indicator" data-index="2" aria-label="Go to slide 3" data-astro-cid-wfe7xcno></button><button class="indicator" data-index="3" aria-label="Go to slide 4" data-astro-cid-wfe7xcno></button> </div> </div>  <script type="module">const s=document.querySelector(".carousel"),l=document.querySelectorAll(".carousel-item"),f=document.querySelector(".prev"),y=document.querySelector(".next"),i=document.querySelectorAll(".indicator");let t=0,c=null;const r=()=>{s.style.transform=`translateX(-${t*100}%)`,i.forEach((e,n)=>{e.classList.toggle("active",n===t)})},a=()=>{t=(t+1)%l.length,r()},d=()=>{t=(t-1+l.length)%l.length,r()},u=()=>{c||(c=setInterval(a,interval))},o=()=>{c&&(clearInterval(c),c=null)};f.addEventListener("click",()=>{o(),d()});y.addEventListener("click",()=>{o(),a()});i.forEach((e,n)=>{e.addEventListener("click",()=>{o(),t=n,r()})});let h=0,v=0;s.addEventListener("touchstart",e=>{h=e.changedTouches[0].screenX});s.addEventListener("touchend",e=>{v=e.changedTouches[0].screenX,m()});const m=()=>{const n=h-v;Math.abs(n)>50&&(n>0?a():d())};autoPlay&&u();document.addEventListener("visibilitychange",()=>{document.hidden?o():autoPlay&&u()});</script> <h3>Getting Started with Apiary</h3> <p>Users can get started with our Command Line Interface (CLI) tool. The CLI tool provides commands to automatically deploy (and destroy) Apiary infrastructure using AWS Cloud Development Kit (CDK). Once the user's infrastructure is deployed, they can navigate to the observability and management dashboard to configure their desired settings.</p> <p>Once configured, the Apiary Software Development Kit (SDK) can be used to send client requests to the system.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/04-15-apiary-sdk.png" alt="Apiary SDK" class="image" loading="lazy" data-src="/images/04-15-apiary-sdk.png" data-alt="Apiary SDK" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Apiary SDK </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   </section><section id="building-apiary"> <h2>Building Apiary</h2> <h3 id="serverless-architecture">Serverless Architecture</h3> <h4>The Challenge</h4> <p>Our first major decision was selecting an initial architecture that would serve as a solid foundation for Apiary. We needed a solution that would present client applications with a simple, secure, and consistent interface - in the quickly evolving LLM landscape, the architecture should allow for updates on the backend without changes in how client applications interact with Apiary. The architecture needed to provide flexibility, enabling us to easily integrate features like authentication, dynamic routing, and observability. Additionally, we wanted a scalable solution that would accommodate applications with variable workflows.</p> <div class="info-box"> <h4>Technical Overview</h4> <p>API Gateways serve as centralized entry points for client applications to access backend services or APIs. Clients send requests to the API Gateway, which routes incoming requests to appropriate backend services and handles common concerns such as authentication, rate limiting, and request/response transformation. Backend services can be adjusted without changing how client applications access them through the API Gateway.</p> <p>API Gateways can integrate with various backend architectures, including:</p> <ul> <li><strong>Traditional servers (e.g., EC2)</strong>: These offer complete control and flexibility, but require upfront provisioning, manual scaling to accommodate variable traffic, ongoing management, and fixed costs even during periods of low demand.</li> <li><strong>Container-based solutions (e.g., ECS/EKS)</strong>: Containers provide greater portability and scalability compared to traditional servers. However, they also introduce deployment complexity, management overhead, and typically higher baseline costs.</li> <li><strong>Serverless computing</strong>: In serverless computing models, cloud providers dynamically manage the allocation and provisioning of servers. This means that serverless architectures automatically scale based on demand and don't require upfront provisioning.</li> </ul> </div> <h4>Our Implementation Choices</h4> <p>We chose Amazon API Gateway paired with AWS Lambda as the foundational components for Apiary. API Gateway's built-in capabilities - such as acting as a "front door" for backend APIs and centralizing management of key functions like authentication and rate limiting - made it a natural choice.</p> <p>Behind this gateway, AWS Lambda handles dynamic workloads by running functions that execute only when triggered by incoming requests. AWS Lambda's pay-per-use pricing model means we incur costs only for actual usage, making it efficient for periods of low traffic. Additionally, Lambda's automatic scaling ensures that our system can adapt to sudden spikes in demand without manual intervention.</p> <h4>Tradeoffs and Considerations</h4> <p>Adopting a serverless architecture did involve certain tradeoffs. AWS Lambda has inherent limitations, notably the "cold start" latency - a brief initial delay (around 200ms) encountered when invoking a Lambda function after a period of inactivity. However, after this initial invocation, subsequent requests benefit from a "warm" Lambda instance, significantly reducing latency. Another limitation is Lambda's maximum execution duration of 15 minutes per request. For our typical LLM interactions, which generally complete within seconds, this rarely poses any practical issues.</p> <p>Ultimately, choosing a serverless foundation allowed us to prioritize our development efforts on Apiary's unique capabilities rather than infrastructure management, setting a flexible and efficient base for ongoing work.</p> <h3 id="request-response-transformation">Request/Response Transformation</h3> <h4>The Challenge</h4> <p>As we've discussed, each LLM provider implements their API differently. The variations in parameter naming, request structure, and response formats must be handled in order to build a unified API. The transformation logic can become complex, but more importantly, as providers update their APIs, the logic in the model gateway must be updated as well.</p> <p>There are lots of opportunities for small discrepancies between the providers. For instance, to access the text response from OpenAI, you need to access the <code>content</code> property of the <code>message</code> object of the first element of the <code>choices</code> array in the response object. For Anthropic's Claude models, you need to access the <code>text</code> property in the first element of the <code>content</code> array in the response object. As another example, with OpenAI, if you want to include a developer prompt, you include the <code>"developer"</code> role in the input messages array sent in the request. To do the same with Claude, you must use the top-level <code>system</code> parameter — there is no <code>"system"</code> role for input messages in the Messages API. If you include a <code>"system"</code> role in your input messages, an error is thrown.</p> <h4>Our Implementation</h4> <p>After evaluating various approaches, we chose to wrap an <strong>established library</strong>. We built our core transformation logic around TokenJS, an open-source library that normalizes interactions with LLM providers.</p> <p>Wrapping a library offered several advantages:</p> <ul> <li><strong>Reduced maintenance burden</strong> - The wrapped library stays up-to-date with API changes from almost all of the prominent providers. Wrapping a library means that we're relying on the library maintainers to make any necessary changes rather than trying to keep track of each individual change ourselves.</li> <li><strong>Immediate access to many providers</strong> - The library already supports a wide range of LLM providers.</li> <li><strong>Faster development</strong> - We could focus on higher-level features rather than low-level API integration.</li> </ul> <p>In our initial benchmarking, we found that wrapping the library added some latency. For example, a simple call to Claude 3.5 Sonnet took ~2500ms via the wrapper, compared to ~1250ms with a direct call. However, this was acceptable to us given that the typical latency range for LLM calls are 5000ms or more <sup><a href="#footnote-4">4</a></sup>. This tradeoff was worth it to us for the improvement in maintainability and development speed.</p> <h4>Alternative Approaches and Tradeoffs</h4> <p>In making our decision to use TokenJS, we considered two alternatives: using the built-in request/response mapping templates in AWS's API Gateway, and building a custom transformation layer from scratch. We found the API Gateway transformations to be slightly restrictive and difficult to configure, so we quickly moved on to consider creating our own transformation layer. While we were initially drawn to this option for the control and flexibility, we realized that in such a quickly changing space, we should take advantage of a library that handles any updates for us. The other side of this coin is that our decision creates a dependency risk - should the TokenJS library maintainers fall behind on updates or abandon the project, our model gateway would also fall behind. Ultimately, we felt the reduction in development effort and maintenance overhead was worth some risk that the TokenJS library becomes outdated.</p> <h3 id="api-key-management">API Key Management</h3> <h4>The Challenge</h4> <p>Working with multiple LLM providers requires managing separate API keys for each provider. This presents several challenges for development teams, particularly for teams where multiple developers need access to these credentials.</p> <p>Keys must be securely stored to prevent unauthorized access. Any compromise of these keys could lead to unauthorized usage, costs, and potential data exposure. Key rotation should be possible without service disruption. Additionally, any key management solution should be able to accommodate adding (or removing) LLM providers and securely sharing access with new team members.</p> <h4>Our Implementation</h4> <p>For Apiary, we implemented a two-tier key management strategy that balances security with usability.</p> <p>Our provider API key management system uses AWS Secrets Manager to securely store all LLM provider API keys. Lambda functions retrieve the necessary keys when processing requests, ensuring that keys are not exposed to client applications and reducing the risk of key leakage.</p> <p>Requests from client applications to Apiary are authenticated with a single API key issued by Apiary. Different usage plans associated with different API keys, and with varying rate limits, can be created for different clients. This approach simplifies key management for client application developers - they only need to manage a single API key, regardless of how many LLM providers are used behind the scenes. LLM providers can be added without sharing provider keys with individual developers - developers can simply use their Apiary API key. LLM provider keys can be automatically rotated in Secrets Manager, ensuring continuous application operation.</p> <p>This approach maintains security for provider credentials while providing a simple, unified authentication mechanism for client applications.</p> <h4>Alternatives Approaches and Tradeoffs</h4> <p>It's not uncommon for teams to store API keys in a <code>.env</code> file, but we didn't consider this approach due to security risks and impracticality for production environments or team-based workflows. Another alternative was using an external tool like HashiCorp Vault, but that would introduce operational complexity and management overhead we didn't need. Given that we were already in the AWS ecosystem, Secrets Manager was a natural fit.</p> <h3 id="managing-conversation-history">Managing Conversation History</h3> <h4>The Challenge</h4> <p>Many LLM applications require maintaining the context of ongoing conversations, or threads, which presents several challenges, the most fundamental being that each LLM request is stateless by default. When conversation history is required for context, the entire history must be provided with each request. This requires that we not only store and access the conversation history for each request, but also transform it either before storage or upon retrieval to ensure that different providers can parse it. Given that we had already addressed the issue of varying request and response structures and roles with the TokenJS library, this challenge was simplified in that respect.</p> <p>A complication worth mentioning is that as the conversation grows, the requests to the LLMs grow in size as well, increasing the token count and associated cost. Long conversations may also lead to a loss of context, as there is an upper limit to the amount of context an LLM can support.</p> <p>Managing threads effectively requires persistent storage, standardization across providers, and strategies for handling long-running conversations.</p> <h4>Our Implementation Choices</h4> <p>We implemented a thread management system with the following components:</p> <ul> <li>Each query and response is stored in a DynamoDB table</li> <li>Conversations are identified by a thread ID, which can be provided by the client in the request or auto-generated based on timestamp</li> <li>Standardization via our wrapped library simplified handling different provider formats and roles</li> <li>Upon receiving a query with a thread ID, the table is checked for message history</li> <li>Historical messages are retrieved and passed to the chat completions call for context</li> <li>This approach works across providers despite their different underlying APIs</li> </ul> <p>This implementation provides thread management without requiring clients to manage conversation history themselves. The conversation context is automatically maintained and provided to each new request in the thread.</p> <h4>Alternative Approaches and Tradeoffs</h4> <p>In formulating our implementation of thread management, we considered a couple of other approaches. While exploring other model gateways, we found that Portkey has separate APIs for thread and chat completion. As users, this felt clunky and complicated to us, so we decided a single API for a chat completion, whether it's part of an ongoing conversation or not, was a better approach.</p> <p>Another approach would have been to use each provider's thread API, where available. While this might seem like a logical fit, it would have introduced major complications. For one, not all providers offer this feature, so we would have been limited in which providers we could support. Secondly, if a conversation started with one provider, we would need to retrieve the thread history from their system and reformat it before routing the next message to another provider. This would have complicated one of our main functionalities - multi-provider routing. By managing the thread history ourselves in a normalized format, we retained control of the storage and retrieval of the conversation history and simplified cross-provider functionality.</p> <p>In considering how to handle the effects of a thread that has grown too long, we explored several options. We could have implemented a configurable limit on thread history, either based on the number of exchanges or token count. We also discussed potential responses when that limit is reached - perhaps leveraging an LLM to summarize previous conversation content, or simply returning an error code and requiring users to start a new conversation. Ultimately, given the complexity and variety of potential use cases, we decided to delegate this responsibility to Apiary users, allowing them to implement thread management strategies that best suit their specific needs. This remains an area we're interested in developing further in future work.</p> <h3 id="configurable-routing">Configurable Routing</h3> <h4>The Challenge</h4> <p>As we've discussed, routing requests to the appropriate LLM based on an application's specific needs is one of the paramount features of a model gateway. Thoughtful routing configuration will reduce downtime, minimize costs, create an ideal setup for development and testing, and produce better responses.</p> <p>The primary challenge in implementing a configurable routing system stemmed from our desire for exceptional flexibility. The core difficulty lay in designing a system that could elegantly handle a multi-layered decision tree. Our routing system needed to support:</p> <ul> <li>Request-level provider and model specification</li> <li>Configuration-level defaults</li> <li>Conditional routing based on request metadata</li> <li>Traffic distribution within condition groups</li> <li>Multiple layers of fallback mechanisms</li> </ul> <p>The challenge wasn't simply implementing each feature in isolation, but determining the ideal flow of a request and then creating a cohesive system where these options could interact predictably. When a request enters the gateway, it triggers a cascade of routing decisions, each with its own set of rules and fallback behaviors.</p> <div class="info-box"> <h4>Technical Overview</h4> <p>Potential use cases of <strong>conditional routing</strong> might be to route queries with a <em>user-type</em> header of <em>pro</em>, for example, to better, more expensive models, or queries with a <em>department</em> header of <em>legal</em> to specialized models trained on contracts.</p> <p><strong>Fallback Mechanisms</strong> are essential, as LLM services experience higher-than-typical downtime. Number of retries and status codes to fallback on are also configurable through Apiary.</p> <p><strong>Load Balancing</strong> supports experimental needs, setting users up for canary deployments and A/B testing with no additional configuration.</p> </div> <h4>Our Implementation</h4> <p>It was important to us that, in choosing to use Apiary, developers did not have to sacrifice control over request routing. We implemented a routing system with the goal of providing as much flexibility as possible.</p> <p><strong>Configuration Object</strong>: Routing rules are defined in JSON and stored in an S3 bucket. Our default configuration file is loaded upon deployment, with the option for users to update the configuration through the UI.</p> <p><strong>Fallback Mechanisms</strong>: The system handles transient failures through configurable automatic retries with exponential backoff. Users can specify fallback models at multiple levels (per condition and globally) and define which error status codes should trigger these fallbacks.</p> <p><strong>Load Balancing</strong>: Our implementation supports percentage-based distribution across providers, enabling controlled rollouts through A/B testing and canary deployments.</p> <p><strong>Granular Control</strong>: The system processes routing metadata from request headers. This creates a pathway for specialized routing needs, such as directing specific user segments to appropriate models.</p> <p>This implementation allows for sophisticated routing strategies while maintaining a simple API for clients. The routing layer handles the complexity of provider selection, retries, and fallbacks, making these concerns transparent to the application making the request.</p> <h4>Alternative Approaches and Tradeoffs</h4> <p>When designing the routing system, we considered a couple of other options. We could have implemented a more basic approach with predetermined paths. This approach would have certainly been easier to build, but as users of model gateways ourselves, we had experienced the frustration of limited options and knew we wanted complete flexibility. Another option was to use the pre-built routing features of Amazon's API Gateway. This could have saved development time, but would have limited us with respect to implementation. We knew we would require specialized fallback logic, context-aware routing, and eventually in future developments, streaming response management, which API Gateway would not be best suited for.</p> <h3 id="caching">Caching</h3> <h4>The Challenge</h4> <p>As discussed, common developer concerns when working with LLMs include cost and high latency, especially compared to traditional API calls. Response times can vary significantly depending on the model, the length of the input prompt and generated output (measured in tokens), and the current load on the provider's infrastructure <sup><a href="#footnote-5">5</a></sup>. Tracking tools monitoring API response times regularly record latencies exceeding five seconds, even for relatively fast models like OpenAI's GPT-3.5-turbo generating outputs of up to 512 tokens <sup><a href="#footnote-4">4</a></sup>. Tracked response times increase when using more powerful models, and may increase even further when requesting longer outputs.</p> <p>LLM pricing also varies based on the provider and model, but is generally charged based on tokens used. As of this writing, OpenAI's GPT-4.1 model - which it touts as a "model for everyday tasks" - costs $2.00 per million input tokens and $8.00 per million output tokens. o1-pro, one of OpenAI's most advanced models, costs $150.00 per million input tokens and $600.00 per million output tokens. Costs can add up even when using a cheaper model, especially for high traffic applications.</p> <div class="info-box"> <h4>Technical Overview</h4> <p>In a caching system, responses to prior requests can be reused, rather than calling the LLM again. In <strong>simple caching</strong>, a cached response is only returned if the request matches the previous one exactly. For example, the queries "How do bees make honey?" and "How do bees produce honey?" wouldn't be recognized by a simple cache as equivalent requests, even though it might be reasonable to return the same response in this case.</p> <p>Despite the variability of language and relatively low likelihood of identical phrasing between requests, LLM-integrated applications can benefit greatly from a simple cache. For instance, applications with suggested pre-defined prompts, where the exact same prompt might be sent repeatedly, would result in a high number of simple cache hits. A simple cache can be particularly valuable during development, when a cached response may be preferred to avoid the token usage and latency of an LLM call.</p> <p>In contrast, <strong>semantic caches</strong> are able to recognize the meaning of requests - so that queries like "How do bees make honey?" and "How do bees produce honey?" are identified as semantically equivalent and result in a cache hit.</p> <p>Semantic caching uses <strong>vector embeddings</strong>, which are arrays of hundreds or thousands of numbers that represent the characteristics of a piece of data, like a sentence or a paragraph. <strong>Embedding models</strong> have been trained to convert inputs like sentences into vector embedding outputs representing the inputs' features. Embeddings can be compared using <strong>cosine similarity</strong>, a calculation that measures how similar two embeddings are to each other. The higher the cosine similarity, the more similar the embeddings. Vector embeddings can be stored in a category of databases called <strong>vector databases</strong>, which are optimized for storing and searching high-dimensional vector embeddings.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/05-01-embeddings.png" alt="Embeddings" class="image" loading="lazy" data-src="/images/05-01-embeddings.png" data-alt="Embeddings" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Embeddings </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   </div> <h4>Our Implementation</h4> <p>To help mitigate the latency and costs of LLM calls, Apiary implements both simple and semantic caching. Users have the option to enable one, both, or neither of these caches, depending on their needs.</p> <h5>Simple cache</h5> <p>If both caches are enabled, Apiary checks the simple cache first. Due to its simpler nature, the simple cache is able to return a response faster than the semantic cache.</p> <p>Apiary's simple cache uses DynamoDB, which we chose for its simplicity, single-digit millisecond response times, and easy integration with other AWS services. Requests can be cached globally or per user by including a <code>userId</code> in the request. Each entry's "sort key" is created by combining the LLM provider and model used, with the text of the user request. This accommodates cases where developers want a response to be provided by a specific provider and model due to that model's strengths in some area.</p> <p>Cache items are automatically deleted using DynamoDB's built-in time-to-live (TTL) functionality. Developers can specify a TTL in seconds that works best for them.</p> <h5>Semantic cache</h5> <p>If no match is found in the simple cache, Apiary checks the semantic cache next. The semantic caching system follows this process:</p> <ol> <li>A vector embedding for the request is generated.</li> <li>The embedding is compared against previously cached embeddings using cosine similarity.</li> <li>If a match is found in the cache with a cosine similarity above a configurable threshold, then the cached response will be immediately returned to the user.</li> <li>If no match is found above the threshold, then the request will be sent to an LLM.</li> <li>When the LLM responds, its response will be stored with the embedding for the request in the vector database.</li> </ol> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/05-02-semantic-cache.png" alt="Semantic Cache" class="image" loading="lazy" data-src="/images/05-02-semantic-cache.png" data-alt="Semantic Cache" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Semantic Cache </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>Apiary's semantic caching system uses the Amazon Titan Text Embedding v2 model via AWS Bedrock to generate embeddings. Bedrock is fully managed and easy to integrate with other AWS services, making it a convenient choice for our infrastructure. For storing and searching embeddings, Apiary uses AWS OpenSearch vector search. OpenSearch vector search is a vector database and search tool that is also well-suited for searches that filter on metadata. This allows us to filter semantic cache hits based on userId, provider, and model, just as we do in the simple cache. The semantic cache also implements configurable TTL for cache entries.</p> <h4>Alternative Approaches and Tradeoffs</h4> <p><strong>Simple Cache</strong></p> <p>Before choosing DynamoDB for our simple cache, we considered using the built-in API Gateway caching. This approach would require no additional components, but we quickly found that the functionality of this system was too limited for our needs. Making requests to LLMs requires complex prompts that are better suited for sending in the body of HTTP POST requests. The built-in API Gateway caching doesn't allow for accessing the request body. Additionally, it returns the full cached response, including fields like token usage that aren't sensical to return with a cache hit. Implementing a custom cache solution allowed us to customize the response appropriately.</p> <p><strong>Semantic Cache</strong></p> <p>For embedding generation, we considered making API calls to connect to external models (ex. from OpenAI or Anthropic), which would give us access to more options, but this would require an external network connection outside of the AWS ecosystem. We also considered running a local embedding model to avoid the external call, but this would add considerable complexity to our system.</p> <p>For both the simple and semantic caching systems, we also considered using in-memory caches like ElastiCache or MemoryDB with vector search. While these would have faster response times (sub-millisecond for a simple cache with ElastiCache, and estimated in the low tens of milliseconds for MemoryDB with vector search - vs mid to high tens of milliseconds for OpenSearch), they would also introduce additional complexity and cost to the system. Since OpenSearch is already used elsewhere in Apiary (e.g., for guardrails), it made sense to standardize on one tool. LLM users are also accustomed to some delay, so the slightly higher latency of OpenSearch was acceptable for our use case.</p> <h3 id="guardrails">Guardrails</h3> <h4>The Challenge</h4> <p>One difficulty when working with LLMs is that their behavior is non-deterministic. Their responses can sometimes generate inappropriate, inaccurate, or potentially harmful content. To implement some protection against undesired behavior, developers may wish to apply some variation of guardrails at one or more steps in the request and response flow. Determining where guardrails should go and how they should be implemented presented a design challenge in our pursuit to balance the linguistic strengths of an LLM with safety and consistency. To provide some context, we'll provide a technical overview of guardrails before diving into our implementation decisions.</p> <div class="info-box"> <h4>Technical Overview</h4> <p>Guardrails can be implemented at various "intervention levels" in an LLM application, including on the user's query, on any retrieved supplemental information (such as in a RAG), on the LLM's response, and on what is finally returned to the client. At each level, guardrails can determine whether messages should be accepted as is, filtered, modified, or rejected.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/05-03-guardrails.png" alt="Guardrails" class="image" loading="lazy" data-src="/images/05-03-guardrails.png" data-alt="Guardrails" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Guardrails </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   <p>The main techniques for implementing guardrails include:</p> <ol> <li><strong>Rule-based string manipulation</strong>: These guardrails take a string as input, perform checks or string modifications, and determine the next steps. While this approach is typically fast and low-cost, language can be subtle and context-dependent, so rule-based approaches are the least robust.</li> <li><strong>LLM-based metrics</strong>: These use embedding similarity metrics to estimate how similar a message is to target topics. For example, calculating the semantic similarity between user input and problematic content. This requires embedding a group of "utterances," storing them, and chunking and embedding each LLM response to analyze the semantic similarity between the response and the utterances. This approach is more restrictive, but also increases latency and cost.</li> <li><strong>LLM judges</strong>: These guardrails use LLMs themselves to evaluate whether content violates guidelines. While powerful, this adds significant latency and cost to each interaction.</li> <li><strong>Prompt engineering</strong>: This uses carefully designed prompts to steer the LLM away from generating problematic content. When done well, prompt engineering can be very effective, but its effectiveness is largely dependent on an LLM's capacity to accurately interpret instructions. Applications using prompt engineering alone are also susceptible to prompt injection attacks.</li> </ol> <p>In designing guardrails, developers must balance several competing factors as they fit their specific needs: maximizing the capabilities of LLMs, ensuring safety and relevance of responses, minimizing latency, and managing costs.</p> </div> <h4>Our Implementation</h4> <p>As we've discussed, there are a number of factors that need to be tailored for each situation's needs. For this reason, we chose to make our guardrails system highly configurable. We designed Apiary to offer a number of options:</p> <p><strong>Level One Guardrails: Rule-Based String Manipulation</strong></p> <p>Our first tier implements simple rule-based guardrails that match LLM outputs against an array of blocked words included in the configuration object. This approach:</p> <ul> <li>Provides a latency impact of about 300ms</li> <li>Catches obvious violations (explicit profanity, prohibited topics)</li> <li>Can be easily customized with domain-specific blocklists</li> </ul> <p>This approach acts as a basic layer of defense, with minimal added latency and no added cost.</p> <p><strong>Level Two Guardrails: Semantic Similarity Matching</strong></p> <p>Our second tier implements more sophisticated guardrails using semantic similarity:</p> <ul> <li>Uses embeddings to detect semantic similarity to prohibited content</li> <li>Can catch subtle violations that would bypass word-based filtering</li> <li>Leverages our existing OpenSearch infrastructure for embedding storage and similarity calculations</li> <li>Provides more comprehensive protection at the cost of moderately increased latency (1000ms)</li> </ul> <p>This technique computes semantic similarity scores between 3-sentence chunks of inputs/outputs and target problematic content, rejecting the message whenever the score exceeds a specified, configurable threshold.</p> <p><strong>Resend On Violation</strong></p> <p>For cases where guardrails detect violations, we implemented a "resend on violation" option that:</p> <ul> <li>Sends responses that trigger guardrails back to the LLM</li> <li>Includes instructions to generate a new response that avoids the detected violation</li> <li>Incorporates the "LLM judge" technique without adding latency to every request</li> <li>Provides a fallback mechanism when guardrails block legitimate content</li> </ul> <p>As this approach requires an additional LLM call, it adds the highest amount of latency (approximately 2000ms) and cost, but it results in a much more comprehensive safety framework while still returning a useful response. Rather than simply blocking content, it adaptively regenerates responses while avoiding violations.</p> <p><strong>No Guardrails</strong></p> <p>Lastly, our guardrails are completely optional. For users who would prefer to do without any sort of guardrails, the <code>enabled</code> attribute in the <code>guardrails</code> section may be set to <code>false</code> in the configuration object.</p> <h4>Alternative Approaches and Tradeoffs</h4> <p>For the level two guardrails, our current implementation includes prompt and response content together during evaluation. While this approach maintained the context of the question (for instance, an LLM might not restate the topic at hand and simply respond with an undesired affirmation or rejection, which wouldn't be caught), it created a new challenge: if a prompt mentions prohibited content, the entire response might be blocked unnecessarily. The core challenge is striking a balance between:</p> <ul> <li>Not blocking valid responses that refute misinformation or discuss sensitive topics in an acceptable way</li> <li>Catching harmful outputs, even if they're short ("yes") or rely heavily on context from the prompt</li> </ul> <p>We considered a few improvements to this system as opportunities for future refinement:</p> <ul> <li>Determine stance of the response (affirms or rejects), and only block those responses that affirm the prohibited utterances.</li> <li>Add metadata to guardrails such as "block_if_affirms," "block_if_prompt_includes," or "block_if_refutes" to introduce more granular control over enforcement.</li> <li>Apply different semantic similarity thresholds for prompts versus responses, with a higher threshold for prompts, or with a heavier weighting to response similarity in decision-making.</li> </ul> <p>Ultimately, we found our current guardrails system to be a good balance of safety and simplicity. With full configurability, we felt that users of Apiary could fine-tune its behavior to suit their needs.</p> <h3 id="collecting-and-storing-observability-data">Collecting and Storing Observability Data</h3> <h4>Short-term data</h4> <p><strong>The Challenge</strong></p> <p>Apiary collects data on every request passing through the system, categorizing this information into short-term and long-term logs. Short-term logs represent requests from the past 5 days. Developers require near real-time access to these logs to quickly monitor system performance and detect anomalies or errors.</p> <p><strong>Our Implementation</strong></p> <p>We chose to use AWS DynamoDB for short-term log storage. Log data naturally aligns with DynamoDB's key-value structure, allowing each request's log to be stored as a single object. Additionally, DynamoDB's single-millisecond query latency ensures developers can retrieve logs almost instantly, meeting the need for near real-time observability.</p> <p><strong>Alternative Approaches and Tradeoffs</strong></p> <p>We briefly considered using a relational database for storing short-term logs. However, relational databases introduced unnecessary complexity because our log data is self-contained within individual records. The absence of complex relational joins makes a key-value store like DynamoDB a more straightforward and efficient solution.</p> <h4>Long-term data</h4> <p><strong>The Challenge</strong></p> <p>Long-term log storage presents distinct challenges, particularly around efficiently storing and querying large volumes of historical data. Unlike short-term logs, the priority shifts toward facilitating aggregate queries (e.g., calculating averages or grouping data by provider) rather than rapid individual record retrieval. Therefore, our primary considerations here were query efficiency and cost-effective storage.</p> <div class="info-box"> <h4>Technical Overview</h4> <p>Apache Parquet is a columnar storage format that stores data by columns rather than rows, offering significant compression advantages. Columnar compression reduces the required storage space and significantly accelerates analytical queries since irrelevant columns can be skipped during queries <sup><a href="#footnote-6">6</a></sup>.</p> <p>AWS Athena, a serverless query service, enables querying data stored in Amazon S3 using standard SQL. When paired with parquet format, Athena efficiently reads only the required columns. Additionally, parquet files in S3 can be partitioned by specific columns, further optimizing queries by ensuring only necessary partitions are scanned. Bulk parquet data can be split into separate files, and Athena can run queries in parallel on those files.</p> </div> <p><strong>Our Implementation</strong></p> <p>Apiary stores its long-term log data as Apache Parquet files within Amazon S3, leveraging Athena to execute queries. This approach optimizes both storage costs and query performance due to Parquet's compression and the columnar design. Athena provides the option to run optimized aggregate queries on the data through familiar SQL statements.</p> <p><strong>Alternative Approaches and Tradeoffs</strong></p> <p>An alternative that we considered was storing logs in JSON format. JSON's advantages include human readability, easy interoperability with multiple platforms and tools, and suitability for row-based querying (ideal for accessing detailed individual request data). However, JSON lacks Parquet's storage efficiency and optimized query capabilities for large-scale analytics. Given our priorities of aggregate analysis and the potential scale of log storage, Parquet's compression and aggregate query advantages were decisive factors in choosing it over JSON.</p> <h4>Additional Considerations</h4> <p>Short-term logs are periodically transferred from DynamoDB to S3 via a scheduled cron job. This transaction is not atomic - that is, if any part of the function fails then the database does not revert back to its original state. Making this process more ACID compliant is an important area for future work.</p> <p>Additionally, the logging and routing systems are currently tightly coupled. Routing functions directly instantiate or pass logging objects to capture events (e.g., cache hits). While this provides fine-grained event tracking with minimal overhead, it may become a bottleneck under sustained high traffic (e.g., over 100 requests per second). To improve scalability and performance, we could adopt an event-driven architecture where logs are asynchronously published to a streaming service such as AWS Kinesis Data Streams or Firehose, decoupling logging from routing.</p> <h3 id="observability-and-management-dashboard">Observability and Management Dashboard</h3> <p>Apiary includes an observability dashboard which allows developers to toggle between viewing short-term and long-term logs. The dashboard fetches logs from DynamoDB (for short-term logs) or Athena query (for long-term logs). The UI is implemented in React and hosted via S3 Static Website Hosting service.</p> <p>The dashboard also serves as a management hub where users can configure routing logic and guardrails. We considered different options for where to implement this "management hub" - for example by having users pass a configuration object in the SDK, or via CLI commands. Ultimately we felt that the dashboard UI was the most user-friendly option, where we could provide the most intuitive editing interface along with guidance on entering a valid configuration object.</p> <figure class="image-container" data-astro-cid-6kov3kig> <img src="/images/05-04-apiary-architecture.png" alt="Apiary Architecture" class="image" loading="lazy" data-src="/images/05-04-apiary-architecture.png" data-alt="Apiary Architecture" data-astro-cid-6kov3kig> <figcaption class="image-caption" data-astro-cid-6kov3kig> Apiary Architecture </figcaption> </figure> <div id="imageModal" class="modal" data-astro-cid-6kov3kig> <div class="modal-content" data-astro-cid-6kov3kig> <span class="close" data-astro-cid-6kov3kig>&times;</span> <img id="modalImage" alt="Enlarged view" data-astro-cid-6kov3kig> <div id="modalCaption" data-astro-cid-6kov3kig></div> </div> </div>   </section><section id="future-work"> <h2>Future Work</h2> <p>Apiary provides powerful capabilities for developing and managing LLM applications, but there are several areas where we can enhance the platform further. Our team has identified the following areas for improvement:</p> <h3 id="improving-resiliency">Improving Resiliency</h3> <ul> <li>Streamline the complexity of migrating older logs from DynamoDB to Athena to make it ACID compliant. The current process involves many steps, and ideally we’d like to configure the process so that if one step fails, the process halts and rolls back all prior steps to their previous states before trying again.</li> <li>Decouple the routing and logging functionalities</li> </ul> <h3 id="user-querying">User Querying</h3> <ul> <li>Give users the ability to run their own custom queries on the observability data</li> </ul> <h3 id="optimizing-latency">Optimizing Latency</h3> <ul> <li>Add in-memory caches to improve response times. While we made a tradeoff for simplicity over speed with our current caching systems, having an option to use faster in-memory caches might be worth it for certain use cases.</li> <li>Implement real-time streaming to improve the perception of latency</li> </ul> <h3 id="conversation-management">Conversation Management</h3> <ul> <li>Implement prompt management systems for better control and organization</li> <li>Deploy token-based rate limiting to set limits on conversation history</li> <li>When conversation limits are reached:
<ul> <li>Leverage an LLM to summarize previous conversation content</li> <li>Or simply return an error code and require users to start a new conversation</li> </ul> </li></ul> <h3 id="additional-guardrail-features">Additional Guardrail Features</h3> <ul> <li>Introduce more granular control over guardrails:
<ul> <li>Categorize the stance of responses (affirm or reject) and only block those that affirm prohibited utterances</li> <li>Add metadata to guardrails to allow for specific actions for each guardrail</li> <li>Apply different semantic similarity thresholds for prompts versus responses to more heavily weight the response similarity</li> </ul> </li> </ul> <p>By addressing these areas, Apiary can evolve into an even more robust, responsive, and effective platform for managing LLM applications.</p> </section><section id="references" data-astro-cid-nbfgzvfs> <h2 data-astro-cid-nbfgzvfs>References</h2> <p data-astro-cid-nbfgzvfs>This section contains all references cited throughout the case study.</p> <ol class="footnotes" data-astro-cid-vjcby2fn> <li id="footnote-1" data-astro-cid-vjcby2fn> <a href="https://status.openai.com/" target="_blank" rel="noopener noreferrer" data-astro-cid-vjcby2fn>OpenAI Status Page</a> </li><li id="footnote-2" data-astro-cid-vjcby2fn> <a href="https://status.anthropic.com/" target="_blank" rel="noopener noreferrer" data-astro-cid-vjcby2fn>Anthropic Status Page</a> </li><li id="footnote-3" data-astro-cid-vjcby2fn> <a href="https://blog.langchain.dev/langchain-state-of-ai-2023/" target="_blank" rel="noopener noreferrer" data-astro-cid-vjcby2fn>LangChain State of AI 2023</a> </li><li id="footnote-4" data-astro-cid-vjcby2fn> <a href="https://gptforwork.com/tools/openai-api-and-other-llm-apis-response-time-tracker" target="_blank" rel="noopener noreferrer" data-astro-cid-vjcby2fn>GPT for Work - OpenAI API and Other LLM APIs Response Time Tracker</a> </li><li id="footnote-5" data-astro-cid-vjcby2fn> <a href="https://techcommunity.microsoft.com/blog/azure-ai-services-blog/the-llm-latency-guidebook-optimizing-response-times-for-genai-applications/4131994" target="_blank" rel="noopener noreferrer" data-astro-cid-vjcby2fn>Microsoft Tech Community - The LLM Latency Guidebook</a> </li><li id="footnote-6" data-astro-cid-vjcby2fn> <a href="https://www.databricks.com/glossary/what-is-parquet" target="_blank" rel="noopener noreferrer" data-astro-cid-vjcby2fn>Databricks - What is Parquet?</a> </li> </ol>  </section>    </div> </main> </div>  <script type="module">const r=new IntersectionObserver(e=>{e.forEach(t=>{const o=t.target.getAttribute("id"),n=document.querySelector(`a[href="#${o}"]`);t.isIntersecting&&(document.querySelectorAll(".toc-link").forEach(i=>{i.classList.remove("active")}),n?.classList.add("active"))})},{rootMargin:"-20% 0px -80% 0px",threshold:[0,.5,1]});document.querySelectorAll("section[id], h2[id], h3[id]").forEach(e=>{r.observe(e)});document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();const o=this.getAttribute("href"),n=document.querySelector(o);if(n){const s=n.getBoundingClientRect().top+window.pageYOffset-125;window.scrollTo({top:s,behavior:"auto"}),history.pushState(null,"",o)}})});document.addEventListener("DOMContentLoaded",()=>{const e=window.location.hash;e&&setTimeout(()=>{const t=document.querySelector(e);if(t){const i=t.getBoundingClientRect().top+window.pageYOffset-125;window.scrollTo({top:i,behavior:"auto"})}},100)});</script>  </main>  <script type="module">const e=localStorage.getItem("theme");e?document.documentElement.setAttribute("data-theme",e):document.documentElement.setAttribute("data-theme","dark");</script></body></html>